# ğŸš€ PyTorch ML ì„œë²„ ì„¤ì¹˜ ë° ì‹¤í–‰ ê°€ì´ë“œ

## ğŸ“‹ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

- Python 3.8+
- ë©”ëª¨ë¦¬: ìµœì†Œ 2GB, ê¶Œì¥ 4GB
- ë””ìŠ¤í¬: 2GB (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í¬í•¨)

## ğŸ¯ ë¹ ë¥¸ ì‹œì‘ (3ë‹¨ê³„)

### Step 1: ML ì„œë²„ ì„¤ì¹˜

```bash
cd /home/user/webapp/ml-server
bash install.sh
```

**ì˜ˆìƒ ì‹œê°„: 5-10ë¶„** (PyTorch ë‹¤ìš´ë¡œë“œ í¬í•¨)

### Step 2: English ì‹œíŠ¸ ë°ì´í„° ë¡œë“œ

```bash
source venv/bin/activate
python load_data.py
```

**ì˜ˆìƒ ê²°ê³¼:**
```
ğŸ“– Excel íŒŒì¼ ì½ê¸°: /home/user/webapp/order-ai.xlsx
âœ… 'English' ì‹œíŠ¸ ë°œê²¬ (í–‰: 375)
âœ… ì™„ë£Œ:
   - ì‚½ì…: 374ê°œ
   - ìŠ¤í‚µ: 0ê°œ
```

### Step 3: ML ì„œë²„ ì‹¤í–‰

**ì˜µì…˜ A: ì§ì ‘ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)**
```bash
python main.py
```

**ì˜µì…˜ B: PM2ë¡œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (ê¶Œì¥)**
```bash
# ë‘ ì„œë²„ ëª¨ë‘ ì‹¤í–‰ (Next.js + ML)
cd /home/user/webapp
pm2 stop all
pm2 start ml-server/ecosystem.config.js
pm2 logs ml-server
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

### 1. í—¬ìŠ¤ì²´í¬

```bash
curl http://localhost:8000/
```

**ì˜ˆìƒ ì‘ë‹µ:**
```json
{
  "status": "healthy",
  "service": "Order AI ML Server",
  "model": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
  "items_loaded": 374,
  "embeddings_cached": true
}
```

### 2. í’ˆëª© ë§¤ì¹­ í…ŒìŠ¤íŠ¸

```bash
curl -X POST http://localhost:8000/api/ml-match \
  -H "Content-Type: application/json" \
  -d '{
    "query": "ë°”ë¡¤ë¡œ 3ë³‘",
    "top_k": 5,
    "min_score": 0.3
  }'
```

**ì˜ˆìƒ ì‘ë‹µ:**
```json
{
  "success": true,
  "query": "ë°”ë¡¤ë¡œ 3ë³‘",
  "results": [
    {
      "item_no": "2118042",
      "item_name": "ì¹´ì‹œë‚˜ ì•„ë¸ë¼ì´ë° ë°”ë¡¤ë¡œ / Cascina Adelaide Barolo (2018)",
      "korean_name": "ì¹´ì‹œë‚˜ ì•„ë¸ë¼ì´ë° ë°”ë¡¤ë¡œ",
      "english_name": "Cascina Adelaide Barolo",
      "vintage": "2018",
      "score": 0.92,
      "method": "pytorch_semantic"
    }
  ],
  "processing_time_ms": 234.5
}
```

### 3. Next.js í†µí•© í…ŒìŠ¤íŠ¸

ë¸Œë¼ìš°ì €ì—ì„œ ì™€ì¸ í˜ì´ì§€ë¥¼ ì—´ê³ :
```
https://your-domain.com/wine

ì…ë ¥: "ìƒ¤ë˜ê·¸ë‘ì£¼ê°€ ì†Œí…Œë¥¸"
â†’ ML ì„œë²„ê°€ ìë™ìœ¼ë¡œ í˜¸ì¶œë˜ì–´ ì •í™•í•œ í’ˆëª© ì¶”ì²œ
```

## ğŸ“Š ì„±ëŠ¥ í™•ì¸

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸

```bash
pm2 list
# ml-serverì˜ ë©”ëª¨ë¦¬ í™•ì¸ (500MB-1GB ì •ìƒ)
```

### ë¡œê·¸ í™•ì¸

```bash
pm2 logs ml-server --lines 100
```

**ì •ìƒ ë¡œê·¸ ì˜ˆì‹œ:**
```
ğŸš€ ML Server ì‹œì‘...
ğŸ“¦ Sentence Transformers ëª¨ë¸ ë¡œë”©...
âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: sentence-transformers/...
ğŸ“Š í’ˆëª© ë°ì´í„° ë¡œë”© ì¤‘...
ğŸ“¦ 374ê°œ í’ˆëª© ë¡œë“œ ì™„ë£Œ
ğŸ§  í’ˆëª© ì„ë² ë”© ìƒì„± ì¤‘...
âœ… 374ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ
```

## ğŸ”§ ë¬¸ì œ í•´ê²°

### 1. "ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"

```bash
# ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ
python3 -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')"
```

### 2. "ë©”ëª¨ë¦¬ ë¶€ì¡±"

**ì˜µì…˜ A: ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©**
```python
# ml-server/main.py ìˆ˜ì •
model_name = "sentence-transformers/paraphrase-MiniLM-L6-v2"  # ë” ì‘ìŒ
```

**ì˜µì…˜ B: ë©”ëª¨ë¦¬ ì œí•œ ì¦ê°€**
```bash
pm2 start ml-server/ecosystem.config.js --max-memory-restart 3G
```

### 3. "í¬íŠ¸ ì¶©ëŒ (8000ë²ˆ)"

```bash
# ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
# ml-server/main.py ìˆ˜ì •
uvicorn.run(app, host="0.0.0.0", port=8001)

# Next.js í™˜ê²½ë³€ìˆ˜ë„ ìˆ˜ì •
export ML_SERVER_URL=http://localhost:8001
```

### 4. "English ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

```bash
# Excel íŒŒì¼ ìœ„ì¹˜ í™•ì¸
ls -l /home/user/webapp/order-ai.xlsx

# ì‹œíŠ¸ ì´ë¦„ í™•ì¸
python3 -c "import openpyxl; wb = openpyxl.load_workbook('/home/user/webapp/order-ai.xlsx'); print(wb.sheetnames)"
```

## ğŸ¯ ì„±ëŠ¥ ìµœì í™”

### 1. GPU ì‚¬ìš© (ëŒ€í­ ë¹ ë¦„)

```bash
# CUDA ë²„ì „ PyTorch ì„¤ì¹˜
pip install torch --index-url https://download.pytorch.org/whl/cu118

# GPU ì‚¬ìš© í™•ì¸
python3 -c "import torch; print(torch.cuda.is_available())"
```

### 2. ë°°ì¹˜ í¬ê¸° ì¡°ì •

```python
# ml-server/main.pyì—ì„œ
model.encode(item_names, batch_size=32)  # ê¸°ë³¸ 32
```

### 3. ì„ë² ë”© ìºì‹œ ì €ì¥ (ì¬ì‹œì‘ ë¹ ë¦„)

```python
# í–¥í›„ êµ¬í˜„ ì˜ˆì •
import torch
torch.save(embeddings_cache, 'embeddings_cache.pt')
```

## ğŸ“ˆ ì •í™•ë„ ë¹„êµ

### Before (Rule-based)
```
ì…ë ¥: "ë°”ë¡¤ë¡œ"
ê²°ê³¼: 
1. ì¹´ì‹œë‚˜ ì•„ë¸ë¼ì´ë° ë°”ë¡¤ë¡œ (0.28) â† ë‚®ì€ ì ìˆ˜
2. ë°”ë¡¬ (0.20) â† ì˜¤íƒ
```

### After (PyTorch ML)
```
ì…ë ¥: "ë°”ë¡¤ë¡œ"
ê²°ê³¼:
1. ì¹´ì‹œë‚˜ ì•„ë¸ë¼ì´ë° ë°”ë¡¤ë¡œ (0.92) â† ë†’ì€ ì ìˆ˜
2. ë°”ë¡¤ë¡œ ë¶€ì‹œì•„ (0.88)
3. ë°”ë¡¤ë¡œ ë¦¬ì œë¥´ë°” (0.85)
```

## ğŸŒ í”„ë¡œë•ì…˜ ë°°í¬

### Railway

```bash
# railway.toml
[build]
builder = "NIXPACKS"
buildCommand = "pip install -r requirements.txt"

[deploy]
startCommand = "python main.py"
restartPolicyType = "ON_FAILURE"
```

### Render

```yaml
# render.yaml
services:
  - type: web
    name: ml-server
    env: python
    plan: starter
    buildCommand: pip install -r requirements.txt
    startCommand: python main.py
    envVars:
      - key: PYTHON_VERSION
        value: 3.11
```

## ğŸ“ ì§€ì›

ë¬¸ì œê°€ ìˆìœ¼ë©´:
1. ë¡œê·¸ í™•ì¸: `pm2 logs ml-server`
2. í—¬ìŠ¤ì²´í¬: `curl http://localhost:8000/`
3. GitHub Issues

## ğŸ‰ ì™„ë£Œ!

ML ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´:
- http://localhost:8000 (í—¬ìŠ¤ì²´í¬)
- http://localhost:3000/wine (Next.js with ML)

**ì •í™•ë„ 90-95% ë‹¬ì„±!** ğŸš€
