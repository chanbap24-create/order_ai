"""
PyTorch + Sentence Transformers ê¸°ë°˜ í’ˆëª© ë§¤ì¹­ API
ì •í™•ë„ ìµœìš°ì„  - 90-95% ëª©í‘œ
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import torch
from sentence_transformers import SentenceTransformer, util
import sqlite3
import os
from datetime import datetime

app = FastAPI(
    title="Order AI - ML Matching Server",
    description="PyTorch ê¸°ë°˜ í’ˆëª© ë§¤ì¹­ ì„œë²„ (ì •í™•ë„ ìµœìš°ì„ )",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” êµ¬ì²´ì ì¸ ë„ë©”ì¸ ì§€ì •
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
model = None
db_path = None
items_cache = None
embeddings_cache = None

# ==================== Pydantic Models ====================

class Item(BaseModel):
    item_no: str
    item_name: str
    korean_name: Optional[str] = None
    english_name: Optional[str] = None
    vintage: Optional[str] = None

class MatchRequest(BaseModel):
    query: str
    client_code: Optional[str] = None
    top_k: int = 5
    min_score: float = 0.3

class MatchResult(BaseModel):
    item_no: str
    item_name: str
    korean_name: Optional[str] = None
    english_name: Optional[str] = None
    vintage: Optional[str] = None
    score: float
    method: str = "pytorch_semantic"

class MatchResponse(BaseModel):
    success: bool
    query: str
    results: List[MatchResult]
    processing_time_ms: float
    model_info: Dict[str, str]

# ==================== ì´ˆê¸°í™” ====================

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ ë° ì´ˆê¸°í™”"""
    global model, db_path, items_cache, embeddings_cache
    
    print("ğŸš€ ML Server ì‹œì‘...")
    print("ğŸ“¦ Sentence Transformers ëª¨ë¸ ë¡œë”©...")
    
    # ë‹¤êµ­ì–´ ëª¨ë¸ ë¡œë“œ (í•œêµ­ì–´-ì˜ì–´ ìµœì í™”)
    # Option 1: ë‹¤êµ­ì–´ ìµœê°• ëª¨ë¸ (ê¶Œì¥)
    model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    
    # Option 2: í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ (í•œêµ­ì–´ë§Œ ì²˜ë¦¬í•  ê²½ìš°)
    # model_name = "jhgan/ko-sroberta-multitask"
    
    try:
        model = SentenceTransformer(model_name)
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise
    
    # DB ê²½ë¡œ ì„¤ì •
    db_path = os.path.join(os.path.dirname(__file__), "..", "data.sqlite3")
    if not os.path.exists(db_path):
        print(f"âš ï¸ DB íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {db_path}")
        print("   English ì‹œíŠ¸ ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë¡œë“œí•©ë‹ˆë‹¤...")
        await preload_items()
    else:
        print(f"âœ… DB ì—°ê²°: {db_path}")
        await preload_items()

async def preload_items():
    """í’ˆëª© ë°ì´í„° ë¯¸ë¦¬ ë¡œë“œ ë° ì„ë² ë”© ìƒì„±"""
    global items_cache, embeddings_cache
    
    print("ğŸ“Š í’ˆëª© ë°ì´í„° ë¡œë”© ì¤‘...")
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # client_item_statsì—ì„œ ê³ ìœ  í’ˆëª© ë¡œë“œ
        cursor.execute("""
            SELECT DISTINCT item_no, item_name 
            FROM client_item_stats 
            WHERE item_no IS NOT NULL AND item_name IS NOT NULL
            LIMIT 1000
        """)
        
        rows = cursor.fetchall()
        conn.close()
        
        if not rows:
            print("âš ï¸ í’ˆëª© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. English ì‹œíŠ¸ íŒŒì‹±ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            items_cache = []
            embeddings_cache = None
            return
        
        # ìºì‹œ ìƒì„±
        items_cache = [
            {"item_no": row[0], "item_name": row[1]}
            for row in rows
        ]
        
        print(f"ğŸ“¦ {len(items_cache)}ê°œ í’ˆëª© ë¡œë“œ ì™„ë£Œ")
        
        # ëª¨ë“  í’ˆëª©ëª…ì˜ ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚° (ì†ë„ ìµœì í™”)
        print("ğŸ§  í’ˆëª© ì„ë² ë”© ìƒì„± ì¤‘...")
        item_names = [item["item_name"] for item in items_cache]
        embeddings_cache = model.encode(item_names, convert_to_tensor=True)
        print(f"âœ… {len(item_names)}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ í’ˆëª© ë¡œë“œ ì‹¤íŒ¨: {e}")
        items_cache = []
        embeddings_cache = None

# ==================== API Endpoints ====================

@app.get("/")
async def root():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "healthy",
        "service": "Order AI ML Server",
        "model": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        "items_loaded": len(items_cache) if items_cache else 0,
        "embeddings_cached": embeddings_cache is not None
    }

@app.post("/api/ml-match", response_model=MatchResponse)
async def match_items(request: MatchRequest):
    """
    í’ˆëª© ë§¤ì¹­ API - PyTorch ì˜ë¯¸ ê¸°ë°˜ ë§¤ì¹­
    
    ì •í™•ë„ ìµœìš°ì„  (90-95% ëª©í‘œ)
    """
    start_time = datetime.now()
    
    if not model:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    if not items_cache or embeddings_cache is None:
        raise HTTPException(status_code=503, detail="í’ˆëª© ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = model.encode(request.query, convert_to_tensor=True)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (GPU ê°€ì†)
        similarities = util.cos_sim(query_embedding, embeddings_cache)[0]
        
        # ìƒìœ„ Kê°œ ê²°ê³¼ ì¶”ì¶œ
        top_results = torch.topk(similarities, k=min(request.top_k * 2, len(items_cache)))
        
        # ê²°ê³¼ í•„í„°ë§ ë° í¬ë§·íŒ…
        results = []
        for idx, score in zip(top_results.indices, top_results.values):
            score_value = float(score)
            
            # ìµœì†Œ ì ìˆ˜ í•„í„°
            if score_value < request.min_score:
                continue
            
            item = items_cache[int(idx)]
            
            # í•œê¸€/ì˜ë¬¸ ë¶„ë¦¬ (í˜•ì‹: "í•œê¸€ëª… / English Name (2018)")
            item_name = item["item_name"]
            korean_name = None
            english_name = None
            vintage = None
            
            if " / " in item_name:
                parts = item_name.split(" / ")
                korean_name = parts[0].strip()
                english_part = parts[1].strip() if len(parts) > 1 else ""
                
                # ë¹ˆí‹°ì§€ ì¶”ì¶œ
                if "(" in english_part and ")" in english_part:
                    vintage_start = english_part.rfind("(")
                    vintage = english_part[vintage_start+1:english_part.rfind(")")]
                    english_name = english_part[:vintage_start].strip()
                else:
                    english_name = english_part
            
            results.append(MatchResult(
                item_no=item["item_no"],
                item_name=item_name,
                korean_name=korean_name,
                english_name=english_name,
                vintage=vintage,
                score=score_value,
                method="pytorch_semantic"
            ))
            
            if len(results) >= request.top_k:
                break
        
        # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        processing_time = (datetime.now() - start_time).total_seconds() * 1000
        
        return MatchResponse(
            success=True,
            query=request.query,
            results=results,
            processing_time_ms=processing_time,
            model_info={
                "name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                "type": "pytorch",
                "multilingual": "true"
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë§¤ì¹­ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/stats")
async def get_stats():
    """ì„œë²„ í†µê³„ ì •ë³´"""
    return {
        "model_loaded": model is not None,
        "items_count": len(items_cache) if items_cache else 0,
        "embeddings_cached": embeddings_cache is not None,
        "cache_size_mb": embeddings_cache.element_size() * embeddings_cache.nelement() / (1024**2) if embeddings_cache is not None else 0
    }

# ==================== ë©”ì¸ ì‹¤í–‰ ====================

if __name__ == "__main__":
    import uvicorn
    
    print("=" * 60)
    print("ğŸš€ Order AI ML Server - PyTorch + Sentence Transformers")
    print("=" * 60)
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )
